# 机器学习-吴恩达

课程网址：https://study.163.com/course/courseMain.htm?courseId=1004570029

作业网址：https://www.coursera.org/learn/machine-learning/home/welcome

笔记网址：http://www.ai-start.com/ml2014/

课后编程题：https://github.com/imLogM/Machine_Learning_AndrewNg

## Week1

### 章节1 介绍

#### 监督学习

给出包含正确答案(连续)的数据集，预测函数的拟合曲线。同样可称为**回归问题。**

给出01值(离散)的数据集，根据变量进行分类。成为**分类问题。**

在特征无限多时能使用算法解决。

#### 无监督学习

数据集中不包含阳性与阴性的区别。不能给出答案，但能用于分类，将不同特征的数据分簇（聚类）

#### Quiz

机器学习是一个研究领域，它使计算机无需明确编程即可学习。

检查大量已知为垃圾邮件的电子邮件，以发现是否存在垃圾邮件的子类型 =>可以用**无监督学习**解决

### 章节2 单变量线性回归

（x^(i), y^(i)）: 第i个样本(x, y)

#### 假设函数hθ(x)

预测函数。hθ(x) = θ0 + θ1x

#### 代价函数 J(θ0，θ1)

目标函数：方差之和*(1/(2m))最小：min (1/(2m))∑(预测值-实际值)^2

代价函数(平方误差函数)J(θ)：(1/(2m))∑(预测值-实际值)^2

#### 梯度下降法

找寻代价函数最小值的算法。

不断找到局部最优下降梯度。向梯度最大的地方移动

* α：学习率，越大每次下降得越多

* 梯度下降公式： θj = θj - α *( J(θ0，θ1)对θj的偏导 )

  ​							(j = 0 或 j = 1，都要计算，同时更新)

​		本质是不停地将当前的θ值减去斜率，不断接近最小值

* 凸函数：向下凹陷

### 章节3 线代

* 向量: nx1的矩阵

* 使用矩阵计算函数的时候，函数参数放在列上，变为向量，变量也一样，均竖着放

* 矩阵乘法不满足交换律、满足结合律

* I:单位矩阵

#### 逆

* AxA^-1 = A^-1xA = I
* 只有mxm的矩阵才有逆
* 所有元素为0的矩阵并没有逆矩阵

#### 转置

* 第一行改为第一列
* 第二行改为第二列
* ...
* B*ji* = A*ij*

## Week2

### 章节5 多变量线性回归

#### 公式

θ = [θ1 θ2...θn]

x = [x1 x2 ... xn]

hθ(x) = θ0 + θ1x1 + θ2x2 + ... + θnxn = θTx

#### 梯度下降

公式改为对每一个θ求偏导，而不仅仅是θ0与θ1 

#### 特征缩放

在量纲不同的情况下，运行梯度下降法可能会导致收敛过程极其漫长，因此需要对数据进行归一化处理

处理时可以直接/最大值，得到-1~1之间的值；也可以先减去平均值再/最大值，得到-0.5~0.5之间的值。

公式：(x1 - u1)/s1 （u1可以为标准差、平均值或0）

#### 学习率

J(θ)曲线如果上升或是不断波动，说明α过大

α经常取值0.001,0.003,0.01,0.03,0.1,0.3,1

#### 多项式非线性回归

特征的选择不一定要使用给出的，可以通过给出的特征计算结果得出选择

h(x) = θ0 + θ1x+ θ2x^2 + + θ3x^3

可以用多变量方法解决，将x^2视为x2，x^3视为x3，变为多变量看待

#### 正规方程法

当J(θ)为能求导得出最小值的函数时，可以用微积分直接求解。

公式：θ = (X^TX)^-1X^Ty

#### 梯度下降与正规方程对比

梯度下降法需要选择α，多次迭代；正规方程法不需要。

梯度下降法当数据量巨大时依然很有效；正规方程法需要计算数据量大小同等的矩阵，n巨大时效率较低。

当维度上万时梯度下降法较为合适。

#### 矩阵不可逆时如何处理正规方程法

无用特征或特征过多时会导致矩阵不可逆，解决方法删除多余特征。

Octave使用pinv能正常计算，无需删除

#### Quiz

1. 您运行梯度下降15次迭代，α= 0.3并计算J（θ）后续迭代。 你会发现J（θ）的值迅速下降然后趋于平稳。

    =>因为迭代次数过少，因此需要减少α的值来增加迭代次数

2. Suppose *m*=4 students have taken some class, and the class had a midterm exam and a final exam. You have collected a dataset of their scores on the two exams, which is as follows:

| midterm exam | (midterm exam)^22 | final exam |
| ------------ | ----------------- | ---------- |
| 89           | 7921              | 96         |
| 72           | 5184              | 74         |
| 94           | 8836              | 87         |
| 69           | 4761              | 78         |

You'd like to use polynomial regression to predict a student's final exam score from their midterm exam score. Concretely, suppose you want to fit a model of the form h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2*h**θ*(*x*)=*θ*0+*θ*1*x*1+*θ*2*x*2, where x_1*x*1 is the midterm score and x_2*x*2 is (midterm score)^22. Further, you plan to use both feature scaling (dividing by the "max-min", or range, of a feature) and mean normalization.

What is the normalized feature x_1^{(3)}*x*1(3)? (Hint: midterm = 94, final = 87 is training example 3.) Please round off your answer to two decimal places and enter in the text box below.

**归一化处理：有两种方式：**
**1.（当前值-平均值）/（最大值-最小值）；**
**2.（当前值-平均值）/方差；**

### 章节6 Octave与Matlab教程

#### 移动数据

`randn(2，3)`:生成符合正态分布的2x3高斯随机变量矩阵

`hist(w1, 50)`:生成w1数据的直方图，条数50

`eye(3)`:生成3x3的单位矩阵

`size(A,1)`:返回矩阵行数

`length(A)`:返回最长维度

`load date.dat`或`load('date.dat')`:读取文件

`who`:显示所有变量

`whos`:显示所有变量且显示详细信息

`clear(w)`:清除变量数据

`v = priceY(1:10)`:将priceY向量的(1:10)赋值给v

`save`:储存为文件

#### 计算数据

`find(a < 3)`：返回a向量中小于3的元素的下标。如果是矩阵会返回行向量与列向量

`a < 3`:返回a向量中每个元素是否小于3的布尔矩阵

`magic`：魔术矩阵，行、列、对角线之和都相等

`prod(a)`:对a中元素全部相乘

`ceil`:向上取整

`floor`:向下取整

`max(A,[],1)`:列最大值

`max(A,[],2)`：行最大值

`max(max(A))`:矩阵最大值 

#### 数据绘制

`pring -dpng 'myPlot.png'`：保存为png

`close`:关闭图像

`figure(1)`:标记图像

`subplot(1,2,1)`:分为1x2的格子，使用第1个

`axis([0.5 1 -1 1])`:改变xy轴显示范围

`imageesc`:将矩阵显示为彩色方块图(表示值大小), `colorbar`:加热度条,`colormap gray`：设计颜色风格

#### 控制语句

```octave
all = 1:10
for i=all,
	# do something
end;

i = 1
while i <= 5,
	# do something
	i = i+1;
end;

while true,
	# do something
	break;
end;

if i == 6,
	# do something
elseif i == 5,
	# do something
else 
	# do something
end
```

#### 函数

创建`.m`文件

```octave
function [y1, y2] = func(x)

y1 = x^2
y2 = x3
```

返回[y1, y2]，需要参数x

`addpath`:添加搜索路径

#### 矢量

求和可转换为矩阵相乘问题

matlab与octave下标从1开始

## Week3

### 章节7 Logistic回归

分类算法。

#### 假设

0 <= hθ(x) <= 1

hθ(x) = g(θTx)

sigmoid函数：g(z) = 1/(1+e^-z)

#### 决策界限

-3 + x1 + x2 >=0

用一条直线划分区域

也可以用各种多项式表示，目的是划分区域

#### 代价函数

* 线性回归cost: cost = 1/2 方差 = 1/2(prediction - y) ^ 2

* 由于计算prediction使用sigmoid函数，会导致梯度下降法无法一定找到全局最优

* Logistic cost:  -log(prediction), y = 1

​						        -log(1-predection), y = 0

* 可简化为 cost = -ylog(prediction) - (1-y)log(1-prediction)
* 同样适用于梯度下降
* 除了梯度下降还能用共轭梯度法等方法找最优解
* 可以用fminunc找最优解，通过函数写出各个θ的偏导与J(θ)并返回，可用于简化梯度下降编写过程

#### 多类别分类

不止01，有1234多种离散值

1. 两两组合，将多类分为多个二元分类
2. 每个分类都有一个分类器
3. 最后选出三个分类器中预测概率最高的作为预测值

#### Quiz

使用其他方法(共轭梯度法等)找最优解的原因不是梯度下降法可能找到局部最优解，是因为梯度下降法有时不够快

### 章节8 正则化

#### 过拟合

欠拟合：高偏差

过拟合：高方差

##### 解决方法

1. 选择尽量少的变量，使用模型选择算法自动选择特征
2. 正则化，保留所有变量，但是改变参数大小。

#### 代价函数

在消除高阶项的影响时，可以在原代价函数中对高阶项加入惩罚项(+10000θ3^3)

加入的惩罚项即为正则化项

正则化项：λ(θj^2从j = 1到n的和)，不惩罚θ0

为了避免多个参数都为0，因此λ的选择十分重要

#### 线性回归正则化

在原有的θ更新过程后+(λ/m)*θj

将式子归并后，可发现θj在每次更新都会缩小一点

在使用正规方程法求解的时候需要在 XTX后+λ*θ0位置为0的单位矩阵

λ>0，不会有矩阵不可逆问题

#### Logistic回归正则化

梯度：与线性回归一样在尾部加上惩罚项

用函数(fminunc)计算梯度时：同样，在计算gradient与J(θ)时，尾部加上正则化项

#### Quiz

新加的feature会提高train set的拟合度,而不是example拟合度. 

## Week4

### 章节9 神经网络学习

在n很大时，logistic并不适合非线性预测

神经元通过（树突）接受信息，通过（轴突）输出信息

* 偏置神经元：一直等于1的输入神经元，可写可不写

* 激活函数：指迭代的方程(g(z) = sigmoid(logistic))

* 权重：θ，和模型中的参数是一个东西
* 输入层、隐藏层、输出层都是由神经元组成的
* a^(i)j: 第i层的第j个神经元激活项
* θ^(j): 控制j到j+1层映射的权重矩阵，不是激活函数
* 每个神经元的计算：a^(i)j = g(对应的θ层有由i到上一层所有与i相连的神经元 * 对应神经元的x值)
* θ矩阵的大小为下一层的神经元个数x（当前层神经元个数+1）
* 用z^(i)表示g()中的值，可向量化计算
* 前向传播：从输入层一路向前直到输出层
* 预测时结果和逻辑回归很像，但是特征是经过神经网络映射的

#### 使用神经网络进行复杂非线性规划

重点在于每个神经元的权重(即θ的表示)，使用合适的方式表示值的传递

别忘了添加偏置单元

隐藏层用输出层计算中间结果

通过隐藏层的不断组合，形成更加复杂的函数，最终得出输出层

#### 多元分类

识别对象不止01的分类

* 输出层不止1个，而是与分类的结果类型数量一样多
* 输出的结果是n维向量，结果中对应位置的值是对应的结果

## Week5

### 章节10 神经网络反向传播

#### 代价函数

在多元分类的神经网络里，代价函数是逻辑回归的代价函数每个类别(K)的和

正则化项

#### 反向传播

加入δ^(j)l项，计算j节点在l层的误差

δ^(2) = (θ^(2))Tδ^(3) .* g'(z^(2)) = a^(2) - y^(2)

可表示为前向传播的值a减去目标值y

由于计算δ使用了下一层的δ，因此反向传播

𝛥 ^(𝑚)𝑗𝑘 表示误差矩阵。第 𝑚 层的第 𝑗 个激活单元受到第 𝑘个参数影响而导致的误差。

#### 展开参数（矩阵和向量的切换）

由于fminuc只能使用向量，因此需要将矩阵展开

将三个theta放到同一列里，合为一个大向量。

使用reshape将三个theta向量变为矩阵

#### 梯度检验

在神经网络运行过程中可能会有bug，需要梯度检验

即使用近似值的方法计算梯度，与反向传播的结果比较。

#### 随机初始化

θ初始值全为0在神经网络中无法起到调整权重的效果，因为所有的边权重都一样。最后调整的结果会导致一个神经元的所有输出权重都相等。

用random函数随机化

#### 总体过程

1. 选择神经网络架构，几个输出、隐藏、输出层（常用1个隐藏层，多个的情况下隐藏层每层神经单元个数相等，一般不必输入小）
2. 初始化随机权重(接近0，但不是0)
3. 计算predictions
4. 计算代价函数
5. 运行反向传播算法，计算代价函数偏导数
6. 使用梯度检验确保反向传播有效
7. 使用反向传播计算得出的偏导最小化代价函数

#### Quiz

Δ*i*j*(2):=Δ*i*j(2)+δ*i(3)∗(*a*(2))*j*向量化结果为Δ(2):=Δ(2)+*δ*(3)∗(*a*(2))*T*

神经网络的正则化和逻辑回归一样，加入惩罚值

## Week6

### 章节11 应用机器学习的建议

改善机器学习效果的方法

* 更多数据
* 减少特征
* 增加特征
* 加入多项式特征
* 减少正则化项λ
* 增加λ

#### 评估假设

随机选择7:3训练集与测试集

计算出测试集的代价函数

设定阈值为0.5，计算err观察预测是否准确，最后求平均值

#### 模型选择

训练集、交叉验证集、测试集

1. 使用训练集得出最合适的多项式次数(如x^5)，找到最合适的模型

2. 用交叉验证集验证选择的模型，计算误差，确定参数
3. 最后用测试集评估模型效果

#### 诊断偏差与方差

**高偏差(欠拟合)**情况：训练误差较大，交叉验证与训练误差接近

**高方差(过拟合)**情况：训练误差较小，交叉验证远大于训练误差

#### 正则化与偏差、方差

正则化λ过大 => 欠拟合

正则化λ过小 => 过拟合

选出合适λ：

代价函数增加正则化项，但是三个训练集的误差计算中不加入

改变λ项后，计算训练误差与交叉验证误差，找到交叉验证误差最小处

#### 学习曲线

用于判断学习算法是否处于偏差问题或是方差问题

高偏差的情况下训练误差不会因为数据量变化而减小

高方差训练误差与交叉验证误差差距巨大，增加训练集数据是有效的

#### 如何选择方法

高方差问题：增加训练集、减少特征、增加λ

高偏差问题：增加特征、增加多项式、减少λ

#### Quiz

测试集对正则化参数λ取值进行选择，而非θ